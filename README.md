# Natural Language Inference as a Judge: Detecting Factuality and Causality Issues in Language Model Self-Reasoning for Financial Analysis
## Dataset
`data` stores the three-fold split of samples for factuality and causality detection
- `factuality_fold_1.csv`, `factuality_fold_2.csv`, and `factuality_fold_3.csv` store the 1st, 2nd, and 3rd folds of factuality detection, respectively. The premise corresponds to the input information and the hypothesis is each reasoning statement. Label contains two types of contradiction and entailment.
- `causality_fold_1.csv`, `causality_fold_2.csv`, and `causality_fold_3.csv` store the 1st, 2nd, and 3rd folds of causality detection, respectively. The premise is the reasoning statement and the hypothesis is the classification outcome. Label contains two types of contradiction and entailment.
## Code
- `NLI-as-a-judge.ipynb` records the pre-trained performance of DeBERTa as an example of using the NLI paradigm to detect factual and causal errors in language model self-reasoning content.
- `Backbones.ipynb` compares the detection capability of factual errors across diverse pre-trained backbones using the NLI paradigm.
- `Fine-tuning.ipynb` compares the detection capabilities of pre-trained and fine-tuned versions of DeBERTa in identifying factual and causal errors, demonstrating that fine-tuning significantly enhances model performance, as evidenced by improved p-value significance levels.
- `Self-evaluation bias.ipynb` compares the self-evaluation bias of pre-trained and fine-tuned versions of Gemma-2-2B, demonstrating that fine-tuning significantly mitigates the model's self-evaluation bias, as evidenced by increased p-values indicating a reduced tendency to trust erroneous statements generated by itself.
